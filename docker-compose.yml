services:
  langflow:
    image: langflowai/langflow:latest
    container_name: langflow
    pull_policy: always               # set to 'always' if using 'latest' image
    ports:
      - "7878:7860"
    depends_on:
      - postgres
      - weaviate
      - redis
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
      # This variable defines where the logs, file storage, monitor data and secret keys are stored.
      - LANGFLOW_CONFIG_DIR=app/langflow
    volumes:
      - langflow-data:/app/langflow

  postgres:
    image: postgres:16
    container_name: langflow-postgres
    environment:
      POSTGRES_USER: langflow
      POSTGRES_PASSWORD: langflow
      POSTGRES_DB: langflow
    ports:
      - "5432:5432"
    volumes:
      - langflow-postgres:/var/lib/postgresql/data

  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.28.4
    container_name: langflow-weaviate
    ports:
      - "${WEAVIATE_PORT:-8181}:8080"  # Note external port change
      - "50051:50051"  # Optional gRPC support
    volumes:
      - langflow_weaviate_data:/var/lib/weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 100
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      CLUSTER_HOSTNAME: "node1"
      DEFAULT_VECTORIZER_MODULE: "text2vec-transformers"
      ENABLE_MODULES: "text2vec-transformers"
      TRANSFORMERS_INFERENCE_API: "http://langflow-weaviate-embeddings:8080"

  langflow-weaviate-embeddings:
    image: cr.weaviate.io/semitechnologies/transformers-inference:baai-bge-base-en-v1.5-onnx
    container_name: langflow-weaviate-embeddings
    environment:
      ENABLE_CUDA: "0"

  redis:
    image: redis:7.4 # or use Key DB: eqalpha/keydb:latest
    container_name: langflow-redis
    ports:
      - "6379:6379"
    volumes:
      - langflow_redis-data:/data
    command: redis-server --appendonly yes

  redisinsight:
    image: redislabs/redisinsight:latest
    container_name: langflow-redisinsight
    depends_on:
      - redis
    environment:
      - RI_APP_PORT=6380
    ports:
      - "6380:6380"
    volumes:
      - langflow_redisinsight-data:/db

  hasura:
    image: hasura/graphql-engine:v2.42.0
    container_name: langflow-hasura
    ports:
      - "8182:8080"
    environment:
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgresql://langflow:langflow@langflow-postgres:5432/langflow
      PG_DATABASE_URL: postgresql://langflow:langflow@langflow-postgres:5432/langflow
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_METADATA_DEFAULTS: '{"backend_configs":{"dataconnector":{"athena":{"uri":"http://hasura-data-connector:8081/api/v1/athena"},"mariadb":{"uri":"http://hasura-data-connector:8081/api/v1/mariadb"},"mysql8":{"uri":"http://hasura-data-connector:8081/api/v1/mysql"},"oracle":{"uri":"http://hasura-data-connector:8081/api/v1/oracle"},"snowflake":{"uri":"http://hasura-data-connector:8081/api/v1/snowflake"}}}}'
      HASURA_GRAPHQL_ENABLE_TELEMETRY: "true"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://langflow-otel:4317"
      OTEL_SERVICE_NAME: "hasura-graphql-engine"
      OTEL_TRACES_SAMPLER: "parentbased_always_on"
    depends_on:
      otel:
        condition: service_started
      hasura-data-connector:
        condition: service_healthy

  hasura-data-connector:
    image: hasura/graphql-data-connector:v2.42.0
    container_name: langflow-hasura-data-connector
    ports:
      - "8081:8081"
    environment:
      QUARKUS_LOG_LEVEL: ERROR
      QUARKUS_OPENTELEMETRY_ENABLED: "true"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://langflow-otel:4317"
      OTEL_SERVICE_NAME: "hasura-data-connector"
      OTEL_TRACES_SAMPLER: "parentbased_always_on"
      QUARKUS_OPENTELEMETRY_TRACER_EXPORTER_OTLP_ENDPOINT: "http://langflow-otel:4317"
    depends_on:
      otel:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://langflow-hasura-data-connector:8081/api/v1/athena/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 5s

  otel:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: langflow-otel
    ports:
      - "4317:4317"  # OpenTelemetry gRPC receiver
      - "4318:4318"  # OpenTelemetry HTTP receiver
    logging:
      driver: json-file
      options:
        mode: non-blocking
        max-buffer-size: 4m
    stdin_open: true
    tty: true
    command:
      - "--set=service.telemetry.logs.level=error"
      - "--set=receivers.otlp.protocols.grpc.endpoint=0.0.0.0:4317"
      - "--set=receivers.otlp.protocols.http.endpoint=0.0.0.0:4318"
      - "--set=exporters.debug.verbosity=detailed"
      - "--set=service.pipelines.traces.receivers=[otlp]"
      - "--set=service.pipelines.traces.exporters=[debug]"
      - "--set=service.pipelines.metrics.receivers=[otlp]"
      - "--set=service.pipelines.metrics.exporters=[debug]"
      - "--set=service.pipelines.logs.receivers=[otlp]"
      - "--set=service.pipelines.logs.exporters=[debug]"

volumes:
  langflow-postgres:
  langflow-data:
  langflow_weaviate_data:
  langflow_redis-data:
  langflow_redisinsight-data:

networks:
  default:
    name: ai_net